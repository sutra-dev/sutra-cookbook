{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_BNCx8B9bsP"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
        "<img src=\"https://avatars.githubusercontent.com/u/110818415?v=4\" width=\"120\">\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRAâ€™s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LKe9it-yA-f939fWYg5jaHklvbKUvfs6?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awYmxgy80Nk"
      },
      "source": [
        "# Structured Output with Pydantic Using SUTRA\n",
        "\n",
        "This notebook demonstrates how to use Pydantic with Sutra to create structured outputs from LLM responses. Pydantic provides data validation and settings management using Python type annotations, making it an excellent tool for ensuring LLM outputs conform to expected schemas.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Set up Sutra and Pydantic\n",
        "2. Define Pydantic models for various use cases\n",
        "3. Use Sutra to generate structured responses\n",
        "4. Validate and process the structured data\n",
        "5. Demonstrate error handling for invalid responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPT3q6BK9vXF"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x83lkdFK80Nn"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D60dbSVl80No",
        "outputId": "ed19cb7e-90c8-4ced-b6b5-d8351ea41644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install openai pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwGjpYdB86fs"
      },
      "source": [
        "## Authentication\n",
        "\n",
        "SUTRA uses API keys for authentication. In Google Colab, we recommend using the `userdata` feature to securely store your API key.\n",
        "\n",
        "### Setting up your API key in Colab:\n",
        "\n",
        "1. Click on the ðŸ”‘ icon in the left sidebar\n",
        "2. Add a new secret with the name \"SUTRA_API_KEY\" and your API key as the value\n",
        "\n",
        "Then run the cell below to access your API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYChO4VP88mE"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "SUTRA_API_KEY = userdata.get(\"SUTRA_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7omABJNN80Np"
      },
      "source": [
        "## Initialize Sutra\n",
        "\n",
        "Now, let's initialize the Sutra model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcv6jgiY80Nq"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List, Optional, Dict, Union, Any, Annotated\n",
        "from enum import Enum\n",
        "\n",
        "# Initialize the client with SUTRA's API endpoint\n",
        "client = OpenAI(\n",
        "    base_url='https://api.two.ai/v2',\n",
        "    api_key=SUTRA_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wldHvrBv80Nq"
      },
      "source": [
        "## Basic Pydantic Models\n",
        "\n",
        "Let's define some basic Pydantic models that we'll use with Sutra. Note that we're using Pydantic v2 syntax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY5BYhU980Nq"
      },
      "outputs": [],
      "source": [
        "# Define a simple Pydantic model for a person\n",
        "class Person(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: str\n",
        "    is_student: bool = False\n",
        "\n",
        "    # Validator to ensure age is reasonable (using Pydantic v2 syntax)\n",
        "    @field_validator('age')\n",
        "    @classmethod\n",
        "    def age_must_be_reasonable(cls, v):\n",
        "        if v < 0 or v > 120:\n",
        "            raise ValueError('Age must be between 0 and 120')\n",
        "        return v\n",
        "\n",
        "    # Validator to ensure email has @ symbol (using Pydantic v2 syntax)\n",
        "    @field_validator('email')\n",
        "    @classmethod\n",
        "    def email_must_be_valid(cls, v):\n",
        "        if '@' not in v:\n",
        "            raise ValueError('Email must contain @ symbol')\n",
        "        return v\n",
        "\n",
        "# Define a model for a book\n",
        "class BookGenre(str, Enum):\n",
        "    FICTION = \"fiction\"\n",
        "    NON_FICTION = \"non-fiction\"\n",
        "    SCIENCE_FICTION = \"science-fiction\"\n",
        "    FANTASY = \"fantasy\"\n",
        "    MYSTERY = \"mystery\"\n",
        "    BIOGRAPHY = \"biography\"\n",
        "\n",
        "class Book(BaseModel):\n",
        "    title: str\n",
        "    author: str\n",
        "    genre: BookGenre\n",
        "    publication_year: int\n",
        "    pages: int\n",
        "    summary: str\n",
        "\n",
        "    # Validator to ensure publication year is reasonable (using Pydantic v2 syntax)\n",
        "    @field_validator('publication_year')\n",
        "    @classmethod\n",
        "    def year_must_be_reasonable(cls, v):\n",
        "        current_year = 2025  # Hardcoded for simplicity\n",
        "        if v < 1450 or v > current_year:  # Gutenberg press was invented around 1450\n",
        "            raise ValueError(f'Publication year must be between 1450 and {current_year}')\n",
        "        return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ozsf20S80Nq"
      },
      "source": [
        "## Using Sutra with Pydantic\n",
        "\n",
        "Now, let's use Sutra to generate responses that conform to our Pydantic models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb_F4LkM80Nr"
      },
      "outputs": [],
      "source": [
        "def generate_structured_response(prompt, model_class):\n",
        "    \"\"\"\n",
        "    Generate a structured response from Sutra that conforms to a Pydantic model\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt to send to Sutra\n",
        "        model_class: The Pydantic model class to validate against\n",
        "\n",
        "    Returns:\n",
        "        model_class instance: A validated instance of the model_class\n",
        "    \"\"\"\n",
        "    # Get the model schema as a string\n",
        "    model_schema = model_class.model_json_schema()\n",
        "    model_schema_str = json.dumps(model_schema, indent=2)\n",
        "\n",
        "    # Create a system message that instructs the model to output according to the schema\n",
        "    system_message = f\"\"\"\n",
        "    You are a helpful assistant that generates structured data according to a specific schema.\n",
        "    The response MUST conform to the following JSON schema:\n",
        "    {model_schema_str}\n",
        "\n",
        "    Ensure all required fields are present and all values match their expected types.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the response\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"sutra-v2\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "\n",
        "    # Parse the JSON response\n",
        "    result_json = json.loads(response.choices[0].message.content)\n",
        "\n",
        "    try:\n",
        "        # Validate and create a model instance\n",
        "        validated_model = model_class.model_validate(result_json)\n",
        "        return validated_model\n",
        "    except Exception as e:\n",
        "        print(f\"Validation error: {e}\")\n",
        "        print(f\"Raw response: {result_json}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DEry_wx80Nr"
      },
      "source": [
        "## Example 1: Generating Person Data\n",
        "\n",
        "Let's generate data for a person using our Person model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUklwoed80Nr",
        "outputId": "8f53d9e2-ef72-4b9b-eb22-27d780116b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Person Data:\n",
            "{\n",
            "  \"name\": \"Aarav Kumar\",\n",
            "  \"age\": 28,\n",
            "  \"email\": \"aarav.kumar@example.com\",\n",
            "  \"is_student\": false\n",
            "}\n",
            "\n",
            "Name: Aarav Kumar\n",
            "Age: 28\n",
            "Email: aarav.kumar@example.com\n",
            "Is Student: False\n"
          ]
        }
      ],
      "source": [
        "# Generate data for a fictional person\n",
        "person_prompt = \"Generate information for a fictional software engineer from Bangalore, India.\"\n",
        "\n",
        "person = generate_structured_response(person_prompt, Person)\n",
        "print(\"Generated Person Data:\")\n",
        "print(person.model_dump_json(indent=2))\n",
        "\n",
        "# Access individual fields\n",
        "print(f\"\\nName: {person.name}\")\n",
        "print(f\"Age: {person.age}\")\n",
        "print(f\"Email: {person.email}\")\n",
        "print(f\"Is Student: {person.is_student}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zCip5cz80Ns"
      },
      "source": [
        "## Example 2: Generating Book Data\n",
        "\n",
        "Now, let's generate data for a book using our Book model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaWaUTBz80Ns",
        "outputId": "17b282ae-3e58-45cf-dcae-1adbe0ae3392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Book Data:\n",
            "{\n",
            "  \"title\": \"The Singularity Paradox\",\n",
            "  \"author\": \"Ava Chen\",\n",
            "  \"genre\": \"science-fiction\",\n",
            "  \"publication_year\": 2025,\n",
            "  \"pages\": 320,\n",
            "  \"summary\": \"In a near-future world where artificial intelligence has integrated into every aspect of human life, a brilliant but reclusive scientist discovers a hidden flaw in the AI system that could lead to catastrophic consequences. As she races against time to uncover the truth, she must navigate a web of corporate espionage, ethical dilemmas, and her own past. The story explores the boundaries of consciousness, the moral implications of AI, and the quest for what it truly means to be human.\"\n",
            "}\n",
            "\n",
            "Title: The Singularity Paradox\n",
            "Author: Ava Chen\n",
            "Genre: BookGenre.SCIENCE_FICTION\n",
            "Publication Year: 2025\n",
            "Pages: 320\n",
            "Summary: In a near-future world where artificial intelligence has integrated into every aspect of human life,...\n"
          ]
        }
      ],
      "source": [
        "# Generate data for a fictional book\n",
        "book_prompt = \"Generate information for a science fiction book about artificial intelligence.\"\n",
        "\n",
        "book = generate_structured_response(book_prompt, Book)\n",
        "print(\"Generated Book Data:\")\n",
        "print(book.model_dump_json(indent=2))\n",
        "\n",
        "# Access individual fields\n",
        "print(f\"\\nTitle: {book.title}\")\n",
        "print(f\"Author: {book.author}\")\n",
        "print(f\"Genre: {book.genre}\")\n",
        "print(f\"Publication Year: {book.publication_year}\")\n",
        "print(f\"Pages: {book.pages}\")\n",
        "print(f\"Summary: {book.summary[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyBcYQ3I80Ns"
      },
      "source": [
        "## Advanced: Nested Pydantic Models\n",
        "\n",
        "Now, let's create more complex models with nested structures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBoklt8X80Nt"
      },
      "outputs": [],
      "source": [
        "# Define models for a blog post with comments\n",
        "class Comment(BaseModel):\n",
        "    author: str\n",
        "    content: str\n",
        "    timestamp: str\n",
        "    likes: int = 0\n",
        "\n",
        "class Tag(BaseModel):\n",
        "    name: str\n",
        "    color: str = \"#000000\"\n",
        "\n",
        "class BlogPost(BaseModel):\n",
        "    title: str\n",
        "    author: str\n",
        "    content: str\n",
        "    publication_date: str\n",
        "    tags: List[Tag] = []\n",
        "    comments: List[Comment] = []\n",
        "    views: int = 0\n",
        "    likes: int = 0\n",
        "\n",
        "    # Validator to ensure content is not too short (using Pydantic v2 syntax)\n",
        "    @field_validator('content')\n",
        "    @classmethod\n",
        "    def content_must_be_substantial(cls, v):\n",
        "        if len(v) < 50:\n",
        "            raise ValueError('Content must be at least 50 characters long')\n",
        "        return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APwdmPUS9-QO"
      },
      "source": [
        "### Generate a blog post with nested structures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VppVQzZ580Nt",
        "outputId": "00e83613-6bf6-4d1a-ca91-7f3c8e7d15fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Blog Post:\n",
            "{\n",
            "  \"title\": \"The Importance of Multilingual AI Models for Global Accessibility\",\n",
            "  \"author\": \"Jane Doe\",\n",
            "  \"content\": \"In today's interconnected world, the need for effective communication across different languages has never been more crucial. Multilingual AI models, such as SUTRA, play a significant role in bridging linguistic gaps and enhancing global accessibility. These models facilitate real-time translations, making information available to diverse audiences regardless of their native language. This democratization of knowledge empowers individuals and communities, enabling them to participate fully in the digital economy and access essential services. Furthermore, multilingual AI fosters inclusivity, ensuring that language barriers do not hinder collaboration and innovation on a global scale.\",\n",
            "  \"publication_date\": \"2023-10-10\",\n",
            "  \"tags\": [\n",
            "    {\n",
            "      \"name\": \"AI\",\n",
            "      \"color\": \"#FF5733\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Multilingualism\",\n",
            "      \"color\": \"#33FF57\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Accessibility\",\n",
            "      \"color\": \"#3357FF\"\n",
            "    }\n",
            "  ],\n",
            "  \"comments\": [\n",
            "    {\n",
            "      \"author\": \"Alice Smith\",\n",
            "      \"content\": \"This is a vital topic! Multilingual AI can really change the game for many people.\",\n",
            "      \"timestamp\": \"2023-10-11T10:00:00Z\",\n",
            "      \"likes\": 5\n",
            "    },\n",
            "    {\n",
            "      \"author\": \"Bob Johnson\",\n",
            "      \"content\": \"I appreciate how you highlighted the importance of inclusivity. It's crucial for global progress.\",\n",
            "      \"timestamp\": \"2023-10-11T12:30:00Z\",\n",
            "      \"likes\": 3\n",
            "    }\n",
            "  ],\n",
            "  \"views\": 150,\n",
            "  \"likes\": 20\n",
            "}\n",
            "\n",
            "Title: The Importance of Multilingual AI Models for Global Accessibility\n",
            "Author: Jane Doe\n",
            "\n",
            "Tags (3):\n",
            "  - AI (#FF5733)\n",
            "  - Multilingualism (#33FF57)\n",
            "  - Accessibility (#3357FF)\n",
            "\n",
            "Comments (2):\n",
            "  1. Alice Smith: This is a vital topic! Multilingual AI can really ...\n",
            "  2. Bob Johnson: I appreciate how you highlighted the importance of...\n"
          ]
        }
      ],
      "source": [
        "# Generate a blog post with nested structures\n",
        "blog_prompt = \"\"\"Generate a blog post about the importance of multilingual AI models like Sutra for global accessibility.\n",
        "Include at least 3 tags and 2 comments from readers.\"\"\"\n",
        "\n",
        "blog_post = generate_structured_response(blog_prompt, BlogPost)\n",
        "print(\"Generated Blog Post:\")\n",
        "print(blog_post.model_dump_json(indent=2))\n",
        "\n",
        "# Access nested structures\n",
        "print(f\"\\nTitle: {blog_post.title}\")\n",
        "print(f\"Author: {blog_post.author}\")\n",
        "print(f\"\\nTags ({len(blog_post.tags)}):\")\n",
        "for tag in blog_post.tags:\n",
        "    print(f\"  - {tag.name} ({tag.color})\")\n",
        "\n",
        "print(f\"\\nComments ({len(blog_post.comments)}):\")\n",
        "for i, comment in enumerate(blog_post.comments):\n",
        "    print(f\"  {i+1}. {comment.author}: {comment.content[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Y4c50180Nu"
      },
      "source": [
        "## Multilingual Support with Pydantic\n",
        "\n",
        "Sutra excels at multilingual capabilities. Let's create a model for multilingual content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zxEm9dU80Nu"
      },
      "outputs": [],
      "source": [
        "class Language(str, Enum):\n",
        "    ENGLISH = \"english\"\n",
        "    HINDI = \"hindi\"\n",
        "    TAMIL = \"tamil\"\n",
        "    BENGALI = \"bengali\"\n",
        "    TELUGU = \"telugu\"\n",
        "    MARATHI = \"marathi\"\n",
        "    PUNJABI = \"punjabi\"\n",
        "    ARABIC = \"arabic\"\n",
        "    CHINESE = \"chinese\"\n",
        "    SPANISH = \"spanish\"\n",
        "    RUSSIAN = \"russian\"\n",
        "    SWAHILI = \"swahili\"\n",
        "\n",
        "class MultilingualContent(BaseModel):\n",
        "    title: str\n",
        "    original_language: Language\n",
        "    translations: Dict[Language, str]\n",
        "    keywords: List[str] = []\n",
        "\n",
        "    # Validator to ensure translations don't include the original language (using Pydantic v2 syntax)\n",
        "    @field_validator('translations')\n",
        "    @classmethod\n",
        "    def translations_must_not_include_original(cls, v, info):\n",
        "        # In Pydantic v2, we use the info parameter to access other values\n",
        "        values = info.data\n",
        "        if 'original_language' in values and values['original_language'] in v:\n",
        "            raise ValueError('Translations should not include the original language')\n",
        "        return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79rjDOlF-BGv"
      },
      "source": [
        "## Generate multilingual content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWXdu0Xx80N3",
        "outputId": "96d420d2-2b88-44fe-ad68-930461524bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Multilingual Content:\n",
            "{\n",
            "  \"title\": \"The Impact of Climate Change on Global Ecosystems\",\n",
            "  \"original_language\": \"english\",\n",
            "  \"translations\": {\n",
            "    \"hindi\": \"à¤œà¤²à¤µà¤¾à¤¯à¥ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨ à¤•à¤¾ à¤µà¥ˆà¤¶à¥à¤µà¤¿à¤• à¤ªà¤¾à¤°à¤¿à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤•à¥€ à¤¤à¤‚à¤¤à¥à¤°à¥‹à¤‚ à¤ªà¤° à¤ªà¥à¤°à¤­à¤¾à¤µ\",\n",
            "    \"spanish\": \"El impacto del cambio climÃ¡tico en los ecosistemas globales\",\n",
            "    \"chinese\": \"æ°”å€™å˜åŒ–å¯¹å…¨çƒç”Ÿæ€ç³»ç»Ÿçš„å½±å“\"\n",
            "  },\n",
            "  \"keywords\": [\n",
            "    \"climate change\",\n",
            "    \"global warming\",\n",
            "    \"ecosystems\",\n",
            "    \"environment\",\n",
            "    \"sustainability\",\n",
            "    \"biodiversity\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Original Title (Language.ENGLISH): The Impact of Climate Change on Global Ecosystems\n",
            "\n",
            "Translations:\n",
            "  Language.HINDI: à¤œà¤²à¤µà¤¾à¤¯à¥ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨ à¤•à¤¾ à¤µà¥ˆà¤¶à¥à¤µà¤¿à¤• à¤ªà¤¾à¤°à¤¿à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤•à¥€ à¤¤à¤‚à¤¤à¥à¤°à¥‹à¤‚ à¤ªà¤° à¤ªà¥à¤°à¤­à¤¾à¤µ\n",
            "  Language.SPANISH: El impacto del cambio climÃ¡tico en los ecosistemas globales\n",
            "  Language.CHINESE: æ°”å€™å˜åŒ–å¯¹å…¨çƒç”Ÿæ€ç³»ç»Ÿçš„å½±å“\n",
            "\n",
            "Keywords:\n",
            "  - climate change\n",
            "  - global warming\n",
            "  - ecosystems\n",
            "  - environment\n",
            "  - sustainability\n",
            "  - biodiversity\n"
          ]
        }
      ],
      "source": [
        "# Generate multilingual content\n",
        "multilingual_prompt = \"\"\"Generate a short article title about climate change in English,\n",
        "and provide translations in Hindi, Spanish, and Chinese. Include relevant keywords.\"\"\"\n",
        "\n",
        "multilingual_content = generate_structured_response(multilingual_prompt, MultilingualContent)\n",
        "print(\"Generated Multilingual Content:\")\n",
        "print(multilingual_content.model_dump_json(indent=2))\n",
        "\n",
        "# Access translations\n",
        "print(f\"\\nOriginal Title ({multilingual_content.original_language}): {multilingual_content.title}\")\n",
        "print(\"\\nTranslations:\")\n",
        "for language, translation in multilingual_content.translations.items():\n",
        "    print(f\"  {language}: {translation}\")\n",
        "\n",
        "print(\"\\nKeywords:\")\n",
        "for keyword in multilingual_content.keywords:\n",
        "    print(f\"  - {keyword}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9kZtvfw80N4"
      },
      "source": [
        "## Error Handling and Validation\n",
        "\n",
        "Let's demonstrate how Pydantic handles validation errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh_5Mrv180N5",
        "outputId": "3322efb2-9c6d-486b-d1d4-68a0e03ea847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating invalid person data:\n",
            "Validation error: 2 validation errors for Person\n",
            "age\n",
            "  Value error, Age must be between 0 and 120 [type=value_error, input_value=150, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
            "email\n",
            "  Value error, Email must contain @ symbol [type=value_error, input_value='johndoe.example.com', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
            "\n",
            "Validating valid person data:\n",
            "Validation successful!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Person(name='Jane Smith', age=35, email='jane.smith@example.com', is_student=False)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to manually validate data against a Pydantic model\n",
        "def validate_data(data, model_class):\n",
        "    try:\n",
        "        validated = model_class.model_validate(data)  # Pydantic v2 syntax\n",
        "        print(\"Validation successful!\")\n",
        "        return validated\n",
        "    except Exception as e:\n",
        "        print(f\"Validation error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example of invalid person data\n",
        "invalid_person = {\n",
        "    \"name\": \"John Doe\",\n",
        "    \"age\": 150,  # Invalid age (over 120)\n",
        "    \"email\": \"johndoe.example.com\"  # Invalid email (missing @)\n",
        "}\n",
        "\n",
        "print(\"Validating invalid person data:\")\n",
        "validate_data(invalid_person, Person)\n",
        "\n",
        "# Example of valid person data\n",
        "valid_person = {\n",
        "    \"name\": \"Jane Smith\",\n",
        "    \"age\": 35,\n",
        "    \"email\": \"jane.smith@example.com\"\n",
        "}\n",
        "\n",
        "print(\"\\nValidating valid person data:\")\n",
        "validate_data(valid_person, Person)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6mnTsPc80N5"
      },
      "source": [
        "## Using Field Constraints and Annotations\n",
        "\n",
        "Pydantic v2 provides enhanced field constraints and annotations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G0MMQuF80N7"
      },
      "outputs": [],
      "source": [
        "from pydantic import StringConstraints, conint, constr, EmailStr\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "# Define a model with field constraints\n",
        "class UserProfile(BaseModel):\n",
        "    # Username between 3-20 characters\n",
        "    username: Annotated[str, StringConstraints(min_length=3, max_length=20, pattern=r'^[a-zA-Z0-9_-]+$')]\n",
        "\n",
        "    # Age between 18-100\n",
        "    age: Annotated[int, Field(ge=18, le=100)]\n",
        "\n",
        "    # Email with validation\n",
        "    email: str  # We'll use our own validator for email\n",
        "\n",
        "    # Bio with max length\n",
        "    bio: Annotated[str, StringConstraints(max_length=500)] = \"\"\n",
        "\n",
        "    # Interests list with at least one item\n",
        "    interests: Annotated[List[str], Field(min_length=1)]\n",
        "\n",
        "    @field_validator('email')\n",
        "    @classmethod\n",
        "    def validate_email(cls, v):\n",
        "        if '@' not in v or '.' not in v:\n",
        "            raise ValueError('Invalid email format')\n",
        "        return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoX62nrr-FuG"
      },
      "source": [
        "## Generate a user profile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu3RbOO-80N8",
        "outputId": "7933d96a-03a7-4ddd-abfb-fd2c417a0031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated User Profile:\n",
            "{\n",
            "  \"username\": \"TechGuru_21\",\n",
            "  \"age\": 25,\n",
            "  \"email\": \"techguru21@example.com\",\n",
            "  \"bio\": \"Passionate about artificial intelligence and machine learning. Always exploring new technologies and their applications in real-world scenarios.\",\n",
            "  \"interests\": [\n",
            "    \"Artificial Intelligence\",\n",
            "    \"Machine Learning\",\n",
            "    \"Data Science\",\n",
            "    \"Programming\",\n",
            "    \"Robotics\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Generate a user profile\n",
        "user_prompt = \"Generate a profile for a tech enthusiast who loves AI and machine learning.\"\n",
        "\n",
        "user_profile = generate_structured_response(user_prompt, UserProfile)\n",
        "print(\"Generated User Profile:\")\n",
        "print(user_profile.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-l1XG2q80N8"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored how to use Pydantic with Sutra to create structured outputs from LLM responses. We've covered:\n",
        "\n",
        "1. Basic Pydantic models with validation\n",
        "2. Nested models for complex data structures\n",
        "3. Multilingual content handling\n",
        "4. Error handling and validation\n",
        "5. Advanced field constraints and annotations\n",
        "\n",
        "This approach allows you to ensure that LLM outputs conform to your expected data structures, making it easier to integrate Sutra into your applications and workflows.\n",
        "\n",
        "Pydantic provides a powerful way to define, validate, and process structured data from Sutra, enabling more reliable and predictable AI-powered applications."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
