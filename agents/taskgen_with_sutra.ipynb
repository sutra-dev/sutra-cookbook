{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGzVQmgFqIzH"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
        "<img src=\"https://avatars.githubusercontent.com/u/132088052?s=280&v=4\" width=\"130\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA’s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "  <h2>TaskGen : A Task-based agentic framework building on StrictJSON outputs by LLM agents</h2>\n",
        "  <p>TaskGen is a cutting-edge task-based agentic framework designed for efficient task execution and subtask management, leveraging StrictJSON and native Chain of Thought reasoning. It redefines task automation with advanced features like shared variables, RAG, and async capabilities for streamlined workflows.</p>\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16xQRac5adTjq-J-My8UHRC-tHhutuSH2?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YGVbglOq72c"
      },
      "source": [
        "#SUTRA using TaskGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7DeEutprCZc"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Aey0wCrF00"
      },
      "source": [
        "###Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "frMa2L3of4gz",
        "outputId": "6ab4c048-4aa7-4eb6-9ec1-69942ea15a17"
      },
      "outputs": [],
      "source": [
        "# ✅ Install dependencies\n",
        "!pip install taskgen-ai openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxX4sR8LrK8G"
      },
      "source": [
        "###Configure SUTRA API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7tvMD__g7lB"
      },
      "outputs": [],
      "source": [
        "from taskgen import *\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
        "SUTRA_API_KEY = os.getenv(\"SUTRA_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ8jZh4MrZRL"
      },
      "source": [
        "###Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ed_no4zhUrv"
      },
      "outputs": [],
      "source": [
        "def llm(system_prompt: str, user_prompt: str) -> str:\n",
        "    from openai import OpenAI\n",
        "\n",
        "    client = OpenAI(\n",
        "        api_key=SUTRA_API_KEY,\n",
        "        base_url=\"https://api.two.ai/v2\"  # Sutra's base URL\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model='sutra-v2',  # Use Sutra LLM\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTBMP64xrdoQ"
      },
      "source": [
        "###Example Conversation\n",
        "####Psychology counsellor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5epNrEZhZLC",
        "outputId": "fa170893-e894-4e75-e3a1-b190113fc6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Thoughts: The user has initiated the conversation with a simple greeting, indicating they are open to communication. A friendly and welcoming response is appropriate to encourage further dialogue.\n",
            "Persistent Memory: {'User Requests for the Conversation': 'User is seeking assistance or a topic to discuss.', 'User Emotion': 'Neutral', 'Summary of Key Incidents': 'User initiated conversation with a greeting.'}\n",
            "Summary of Conversation: The user greeted the counsellor, indicating readiness for further discussion.\n",
            "Psychology counsellor: Hello! How can I assist you today? If there’s something on your mind or any specific topic you’d like to discuss, feel free to share.\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "# Psychology Counsellor Agent\n",
        "agent = Agent('Psychology counsellor',\n",
        "              \"Helps to understand and respond to User's emotion and situation. Reply User based on User Requests for the Conversation\",\n",
        "              llm=llm)\n",
        "\n",
        "# Wrap with memory\n",
        "my_agent = ConversationWrapper(agent,\n",
        "             persistent_memory={'User Requests for the Conversation': '',\n",
        "                                'User Emotion': '',\n",
        "                                'Summary of Key Incidents': \"Key incidents relevant to understanding User's situation in one line\"})\n",
        "\n",
        "# Start conversation\n",
        "while True:\n",
        "    user_input = input('User: ')\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    reply = my_agent.chat(user_input)\n",
        "    print(agent.agent_name + ':', reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4_Jf9ZEidi7",
        "outputId": "5ebb74dd-4f60-4d54-fa9c-2e3e4cca12dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['User: hello',\n",
              " 'Psychology counsellor: Hello! How can I assist you today? If there’s something on your mind or any specific topic you’d like to discuss, feel free to share.']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.shared_variables['Conversation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g83PQ2vQigGp",
        "outputId": "f76af913-d0c8-4c53-c1c2-b552a957c2de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'User Requests for the Conversation': 'User is seeking assistance or a topic to discuss.',\n",
              " 'User Emotion': 'Neutral',\n",
              " 'Summary of Key Incidents': 'User initiated conversation with a greeting.'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.shared_variables['Persistent Memory']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNPCia3tilAb"
      },
      "outputs": [],
      "source": [
        "my_agent.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6i2xG9eroQr"
      },
      "source": [
        "###Inventory Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NId_FjFiindd",
        "outputId": "2acddbc9-79ed-4d8a-ac01-84274a42283e"
      },
      "outputs": [],
      "source": [
        "from taskgen.agent import Agent\n",
        "\n",
        "# वस्तु जोड़ने का कार्य\n",
        "def add_item_to_inventory(shared_variables, item: str) -> str:\n",
        "    '''इन्वेंटरी में वस्तु जोड़ता है और क्रिया का परिणाम लौटाता है'''\n",
        "    shared_variables['इन्वेंटरी'].append(item)\n",
        "    return f'{item} को सफलतापूर्वक इन्वेंटरी में जोड़ा गया है।'\n",
        "\n",
        "# वस्तु हटाने का कार्य\n",
        "def remove_item_from_inventory(shared_variables, item: str) -> str:\n",
        "    '''इन्वेंटरी से वस्तु हटाता है और परिणाम लौटाता है'''\n",
        "    if item in shared_variables['इन्वेंटरी']:\n",
        "        shared_variables['इन्वेंटरी'].remove(item)\n",
        "        return f'{item} को सफलतापूर्वक इन्वेंटरी से हटा दिया गया है।'\n",
        "    else:\n",
        "        return f'{item} इन्वेंटरी में नहीं मिला, इसलिए हटाया नहीं जा सका।'\n",
        "\n",
        "# एजेंट बनाएँ\n",
        "agent = Agent(\n",
        "    'इन्वेंटरी प्रबंधक',\n",
        "    'यह एजेंट इन्वेंटरी में वस्तुएँ जोड़ता और हटाता है। केवल वही वस्तुएँ हटाई जा सकती हैं जो इन्वेंटरी में पहले से मौजूद हों।',\n",
        "    shared_variables={'इन्वेंटरी': []},\n",
        "    global_context='इन्वेंटरी: ',\n",
        "    llm=llm\n",
        ").assign_functions([add_item_to_inventory, remove_item_from_inventory])\n",
        "\n",
        "# उदाहरण इनपुट\n",
        "output = agent.run('आलू और टमाटर जोड़ें')\n",
        "print(\"एजेंट का उत्तर:\", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N6Dt8NhiwWK",
        "outputId": "ade80364-4888-4b5c-e17a-034a3846180e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "वर्तमान इन्वेंटरी: ['आलू', 'टमाटर']\n"
          ]
        }
      ],
      "source": [
        "# इन्वेंटरी की वर्तमान स्थिति देखें\n",
        "print(\"वर्तमान इन्वेंटरी:\", agent.shared_variables['इन्वेंटरी'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYxswPsPiy_e"
      },
      "outputs": [],
      "source": [
        "my_agent.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQmJUeFzm3zr",
        "outputId": "bb6a3ac9-b539-489b-97c0-97af03151ac9"
      },
      "outputs": [],
      "source": [
        "from taskgen.agent import AsyncAgent\n",
        "from taskgen.function import AsyncFunction\n",
        "import asyncio\n",
        "\n",
        "# Async wrapper for Sutra LLM\n",
        "async def sutra_llm_async(system_prompt: str, user_prompt: str):\n",
        "    from openai import AsyncOpenAI\n",
        "    client = AsyncOpenAI(api_key=os.getenv(\"SUTRA_API_KEY\"), base_url=\"https://api.two.ai/v2\")\n",
        "    response = await client.chat.completions.create(\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Async LLM function\n",
        "generate_sentence = AsyncFunction(\n",
        "    fn_description=\"Create a cheerful sentence using <number> and <object>\",\n",
        "    output_format={\"output\": \"sentence\"},\n",
        "    fn_name=\"generate_cheerful_sentence\",\n",
        "    llm=sutra_llm_async\n",
        ")\n",
        "\n",
        "# Binary to decimal converter\n",
        "def binary_to_decimal(binary_number: str) -> int:\n",
        "    return int(binary_number, 2)\n",
        "\n",
        "# Create Async Agent\n",
        "my_async_agent = AsyncAgent(\"HappyBot\", \"Creates happy sentences from binary input\", shared_variables={}, llm=sutra_llm_async)\n",
        "my_async_agent.assign_functions([generate_sentence, binary_to_decimal])\n",
        "\n",
        "# Run agent in asyncio loop\n",
        "async def run_async_task():\n",
        "    result = await my_async_agent.run(\"Convert binary 1101 to decimal, then make a cheerful sentence with the number and a kite\")\n",
        "    print(result)\n",
        "\n",
        "await run_async_task()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
