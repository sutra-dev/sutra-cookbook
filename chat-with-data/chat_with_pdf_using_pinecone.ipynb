{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffAin8oNcezx"
      },
      "source": [
        "# üìö Multilingual Chat with PDF (Powered by SUTRA & Pinecone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SM7mV9H_E3T"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
        "<img src=\"https://avatars.githubusercontent.com/u/54333248?s=200&v=4\" width=\"130\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA‚Äôs dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1R6altGWpk6AH30c5RBArJ4isMyAFoifZ?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjbUtmKzAIC0"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQFE3VFcn5R"
      },
      "source": [
        "### üìå 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RELEVLQT-1G",
        "outputId": "82b4bfb2-1976-4f0e-ec8f-a31009aeba4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_openai langchain-community requests pypdf langchain-pinecone ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJnMjlTDcrVe"
      },
      "source": [
        "### üìå STEP 2 : Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA8EaH50UJLX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4JrpGXeYFpA"
      },
      "source": [
        "###üìå  STEP 3 : Load Your PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKIHnFtMYJW5",
        "outputId": "9b27897f-20c1-42e9-b4e2-9fcd1c5c979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 15 pages.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF using PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\")  # Replace with your actual PDF path\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} pages.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdYpd0cqYPyf"
      },
      "source": [
        "###üìå  STEP 4 : Split Document into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-2zEV9YS0R",
        "outputId": "f035080d-00f0-4755-edf2-235b1cc8a2d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 49 chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNuD7EHxYXQN"
      },
      "source": [
        "###üìå  STEP 5 : Set Up Pinecone Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKGs35cMYW4K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = \"sutra-pdf-indexs1\"\n",
        "\n",
        "# Create index if it doesn't exist\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvq3hUPtYj6R"
      },
      "source": [
        "###üìå  STEP 6 : Set Up Pinecone Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OuDdpPKXYm5L",
        "outputId": "69e18bd9-bf81-4b15-90be-74126204539e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bcae0009-2a25-454e-9569-d25bfe67dbab',\n",
              " '0cccd645-a518-4e1a-98ee-e3b38b86e0f0',\n",
              " 'edebf9e5-9c6b-482f-86e4-f59762fbf0a6',\n",
              " 'b254cb1c-cc94-41f7-b793-9136977c18f7',\n",
              " '62e654fb-dfd4-4114-a4f4-327f9d089c0a',\n",
              " '348818b3-65dc-4e46-8d78-6743409a0591',\n",
              " '237bc0b7-973f-40ac-9277-2e696394b65c',\n",
              " '6080e12d-9f22-4a32-97e1-71e0880e0240',\n",
              " 'c96c4758-0dc1-4930-a047-0708a1332591',\n",
              " 'ad30569b-659c-48e5-a5c4-6909654b5720',\n",
              " '138f86cf-79a7-4fbf-8dd2-8bf647f8029d',\n",
              " 'c478ea55-c028-471c-bd05-8f8364db1a5d',\n",
              " '61bc69b5-462a-46e7-a0ff-6790c54e7b8c',\n",
              " 'b8849ac6-8bf5-4719-806f-9c53e549ff3d',\n",
              " '2855d535-7907-4761-aec0-326a4b6f40a8',\n",
              " '029ec78f-cd55-4555-b5c3-c384db18711e',\n",
              " '7dffd71a-7d46-469f-a271-89810ec01d4c',\n",
              " '680101d7-2ac0-43b3-a6d1-7b90bdfd81d0',\n",
              " '9a124643-4fa2-4bfa-8242-43a09524dd83',\n",
              " 'fbeb044a-6b74-40c1-9057-cc4b5830358a',\n",
              " '98e6095f-fc23-41ef-af39-01a1f57941d1',\n",
              " '7e907a4a-4efe-477f-b721-6de0c2c5927b',\n",
              " '01064eaf-8d49-45d1-8682-968a9dc65a33',\n",
              " 'f93227d3-6875-46ee-8ac0-000d9894ec41',\n",
              " '233924f6-38a8-4f2a-a8b2-fa7c7438f84f',\n",
              " '67abde0f-762a-411f-bca6-d54e9fef2026',\n",
              " 'efa881ef-d424-4781-9502-4769d2f49bdc',\n",
              " 'bfcb7ac9-6a0e-4d40-becf-a30ed22f7a45',\n",
              " '64984b24-3dd7-479b-85e9-c75bb88fe6be',\n",
              " '0e6a8d6c-c106-4f6a-bc2f-f30a19d085e7',\n",
              " '24d662d2-efb9-461a-93b7-84cea2446376',\n",
              " 'bbd6b10b-7642-44a3-b930-cf1fe134c828',\n",
              " 'a10575db-258c-4339-a0ea-a6c00bde7554',\n",
              " '3811ef71-80be-4536-8e4f-14461447f9a3',\n",
              " '08ee0e9e-eef3-4fc4-bde3-76855b8b60e1',\n",
              " '489a568c-e445-420d-bf53-90bd726519ed',\n",
              " '9642c9de-8988-4c83-bfc4-58de43a820cc',\n",
              " '844aaa29-8fc0-4e35-8bf1-4644cb23f23e',\n",
              " '1943dff6-74fb-48d1-bec6-2a9e0cad538b',\n",
              " 'f4c3a7c1-1c52-4af3-abb1-4fdf46b380f4',\n",
              " '86b0077e-1c0b-4904-b26b-7b75864ce586',\n",
              " '02813325-e600-4519-992d-3470ec94f931',\n",
              " 'df287f79-ed8e-4bf0-98fc-07c1e1b44ff6',\n",
              " 'd4dbc984-e5b8-4c6c-8e81-dab780e03fef',\n",
              " 'f9f20231-3aa3-45c1-b996-13514c2a5404',\n",
              " '106f1d42-b0d2-41bd-88b2-6fb7e9feb071',\n",
              " '78d4e9ad-c89b-4af4-8dfe-4972a1faf2fc',\n",
              " '865d105c-bbe3-4c28-951e-dbc3f98db481',\n",
              " '37d57f57-cb37-478b-8d4a-99d7de27deec']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Create vector store using Pinecone\n",
        "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "vectorstore = PineconeVectorStore(index, embeddings)\n",
        "\n",
        "# Add documents to Pinecone vector store\n",
        "vectorstore.add_documents(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1tJGZGiYpoH"
      },
      "source": [
        "###üìå  STEP 7 : Set Up Conversation Memory and RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WuhQhUJHYsPm",
        "outputId": "2e71c7a0-7215-4ea9-d7d6-294d6ab01977"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up conversation memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# RAG Chain with Sutra LLM\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(\n",
        "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "        base_url=\"https://api.two.ai/v2\",\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0.7\n",
        "    ),\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFG5DO6QbwFu"
      },
      "source": [
        "###üìå  STEP 8 : Ask Question with Language Specification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URyv_f49b0_A"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Desired language for the response\n",
        "language = \"Hindi\"  # change to any supported language\n",
        "\n",
        "# User question\n",
        "question = \"What is Transformer ?\"\n",
        "\n",
        "# Get RAG answer from chain\n",
        "context_result = rag_chain.invoke({\"question\": question})\n",
        "rag_context = context_result['answer']\n",
        "\n",
        "# Format prompt for multilingual Sutra response\n",
        "system_prompt = f\"\"\"\n",
        "You are a helpful assistant answering based on a document.\n",
        "Use this context: {rag_context}\n",
        "Always reply in: {language}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Invoke Sutra LLM directly for language-controlled response\n",
        "llm = ChatOpenAI(\n",
        "    api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "    base_url=\"https://api.two.ai/v2\",\n",
        "    model=\"sutra-v2\",\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "response = llm.invoke([HumanMessage(content=system_prompt)])\n",
        "print(\"User Question:\", question)\n",
        "print(f\"Assistant ({language}):\", response.content.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OouczSgcR8B"
      },
      "source": [
        "##‚úÖ Finally Integrated UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490,
          "referenced_widgets": [
            "5c91d071cf1045bda8da88d77f1b5517",
            "09dd15bcdf364077ab17373a15c7ded6",
            "984eb21a001746308762c163533322fd",
            "290f09d7608b49e8bee83462f131eee8",
            "66a578fc10ab49908a3482ebe54f647e",
            "3908e1ccc6734f5a80786042716a04cc",
            "a11b5cb437734592a754baa4e777407b",
            "4e615448bba34406a12370bd81138a2c",
            "f2caf040e67440ee94a68701f7e2d639",
            "9cebb2efc0cc4c8f99ab3d17a6f13cb0",
            "955e6b7d8a3544ee80fc4a4705a96f50",
            "f645161c4ad44d8bbf679008ae1c17a7",
            "95bcb26afeff48cf98476299f8c7cee0",
            "b62b7030d7bb4647ab131e4fa1a8b146",
            "369f78bb287b4c5e99e95101c06555ee",
            "633b21e39dff4d2f80b94717ab732a94",
            "766d453a98d34ad8968f49bdfe557ba9",
            "edb92921fcf34abf9b64474393ce3623",
            "4b1c970d0d514a3eb209fb5a37e7639d",
            "63c37b46c9cb49f0b4b9ddfba2def32f",
            "0a4bb3dac19e4b7e859acb7c1f71b6f5",
            "b8d61e89585e4ee29b45a3d7395a9426",
            "354825be71ad4685aec8761425ed4d5f",
            "9cda127e9cf24cd586cab80e98a82701",
            "be62ab4f414e4c50b4b303b58d1c0cea",
            "17c67b31f52a4755b4d69a3b7343eaf5",
            "d6faed134c684328ab8e483344bd1439",
            "9f33bf67125b41ab8617e45c869ec655",
            "171407b520774242bce4f5cc5c87cece"
          ]
        },
        "id": "8HixDKF9cUoc",
        "outputId": "7ff4f27f-3d33-4ecd-b6bf-c86f5628433d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c91d071cf1045bda8da88d77f1b5517",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"<h3 style='font-family:Arial;'>üìö Multilingual Chat with PDF (Sutra + Pinecone)</h3>‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1. Imports\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from tempfile import NamedTemporaryFile\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.schema import HumanMessage\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "\n",
        "# 2. Get Sutra Chat Model\n",
        "def get_sutra_model():\n",
        "    return ChatOpenAI(\n",
        "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "        base_url=\"https://api.two.ai/v2\",\n",
        "        model=\"sutra-v2\",\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "# 3. Load & Index PDF using Pinecone\n",
        "def load_and_index_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Pinecone setup\n",
        "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "    index_name = \"sutra-pdf-indexs2\"\n",
        "\n",
        "    # Create index if it doesn't exist\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "    vectorstore = PineconeVectorStore(index, embeddings)\n",
        "\n",
        "    # Add to index\n",
        "    vectorstore.add_documents(chunks)\n",
        "\n",
        "    # RAG Chain\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=get_sutra_model(),\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "# 4. UI Components\n",
        "pdf_file_upload = widgets.FileUpload(\n",
        "    accept='.pdf',\n",
        "    multiple=False,\n",
        "    description='üìÅ Upload PDF',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "load_pdf_button = widgets.Button(\n",
        "    description=\"üîÑ Load PDF\",\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='150px')\n",
        ")\n",
        "\n",
        "status_output = widgets.Output()\n",
        "\n",
        "languages = [\n",
        "    \"English\", \"Hindi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Telugu\", \"Kannada\",\n",
        "    \"Malayalam\", \"Punjabi\", \"Marathi\", \"Urdu\", \"Assamese\", \"Odia\", \"Sanskrit\",\n",
        "    \"Korean\", \"Japanese\", \"Arabic\", \"French\", \"German\", \"Spanish\", \"Portuguese\",\n",
        "    \"Russian\", \"Chinese\", \"Vietnamese\", \"Thai\", \"Indonesian\", \"Turkish\", \"Polish\",\n",
        "    \"Ukrainian\", \"Dutch\", \"Italian\", \"Greek\", \"Hebrew\", \"Persian\", \"Swedish\",\n",
        "    \"Norwegian\", \"Danish\", \"Finnish\", \"Czech\", \"Hungarian\", \"Romanian\", \"Bulgarian\",\n",
        "    \"Croatian\", \"Serbian\", \"Slovak\", \"Slovenian\", \"Estonian\", \"Latvian\", \"Lithuanian\",\n",
        "    \"Malay\", \"Tagalog\", \"Swahili\"\n",
        "]\n",
        "\n",
        "lang_dropdown = widgets.Dropdown(\n",
        "    options=languages,\n",
        "    value=\"English\",\n",
        "    description='üåê Language:',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "chat_output = widgets.HTML(\n",
        "    value=\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>Chat history will appear here...</div>\"\n",
        ")\n",
        "\n",
        "user_input = widgets.Text(\n",
        "    placeholder='Type your message...',\n",
        "    layout=widgets.Layout(flex='4', width='auto')\n",
        ")\n",
        "\n",
        "send_button = widgets.Button(\n",
        "    description=\"üì§ Send\",\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(flex='1', width='auto')\n",
        ")\n",
        "\n",
        "messages = []\n",
        "conversation_chain = None\n",
        "\n",
        "# 5. Load PDF Logic\n",
        "def on_load_pdf(b):\n",
        "    global conversation_chain\n",
        "    uploaded_files = pdf_file_upload.value\n",
        "\n",
        "    with status_output:\n",
        "        clear_output()\n",
        "        if not uploaded_files:\n",
        "            print(\"‚ùå Please upload a PDF file first.\")\n",
        "            return\n",
        "        try:\n",
        "            print(\"‚è≥ Processing uploaded PDF...\")\n",
        "            uploaded_file = list(uploaded_files.values())[0]\n",
        "            with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "                tmp.write(uploaded_file['content'])\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            conversation_chain = load_and_index_pdf(tmp_path)\n",
        "            print(\"‚úÖ PDF loaded and indexed successfully!\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Error:\", e)\n",
        "\n",
        "load_pdf_button.on_click(on_load_pdf)\n",
        "\n",
        "# 6. Chat Interaction Logic\n",
        "def on_send_click(b):\n",
        "    global conversation_chain\n",
        "    if conversation_chain is None:\n",
        "        with status_output:\n",
        "            clear_output()\n",
        "            print(\"‚ùå Load a PDF first.\")\n",
        "        return\n",
        "\n",
        "    user_text = user_input.value.strip()\n",
        "    if not user_text:\n",
        "        return\n",
        "\n",
        "    lang = lang_dropdown.value\n",
        "    messages.append(f\"<b style='color:#13f22d;'>You:</b> {user_text}\")\n",
        "\n",
        "    context_response = conversation_chain.invoke(user_text)\n",
        "    rag_context = context_response['answer']\n",
        "\n",
        "    system_msg = f\"\"\"\n",
        "You are a helpful assistant answering based on a document.\n",
        "Use this context: {rag_context}\n",
        "Always reply in: {lang}\n",
        "Question: {user_text}\n",
        "\"\"\"\n",
        "\n",
        "    chat_model = get_sutra_model()\n",
        "    sutra_response = chat_model.invoke([HumanMessage(content=system_msg)])\n",
        "    assistant_reply = sutra_response.content.strip()\n",
        "\n",
        "    messages.append(f\"<b style='color:#007acc;'>Assistant ({lang}):</b> {assistant_reply}\")\n",
        "    chat_html = \"<br>\".join(messages)\n",
        "    chat_output.value = f\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>{chat_html}</div>\"\n",
        "    user_input.value = \"\"\n",
        "\n",
        "send_button.on_click(on_send_click)\n",
        "\n",
        "# 7. Final UI Layout\n",
        "input_row = widgets.HBox([user_input, send_button])\n",
        "pdf_row = widgets.HBox([pdf_file_upload, load_pdf_button])\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='font-family:Arial;'>üìö Multilingual Chat with PDF (Sutra + Pinecone)</h3>\"),\n",
        "    pdf_row,\n",
        "    lang_dropdown,\n",
        "    chat_output,\n",
        "    input_row,\n",
        "    status_output\n",
        "])\n",
        "\n",
        "# 8. Display App\n",
        "display(ui)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
