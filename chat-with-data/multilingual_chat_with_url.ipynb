{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqgJRXMePylK"
   },
   "source": [
    "# 📚 Multilingual Chat With URL (Powered by SUTRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MVHp2j6Asvm"
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
    "\n",
    "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "  <h2>SUTRA by TWO Platforms</h2>\n",
    "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA’s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
    "\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1c-qJecFt7c0xcZMMEvF3PHW1e3M0AAFl?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGKd0qeyA1UA"
   },
   "source": [
    "## Get Your API Keys\n",
    "\n",
    "Before you begin, make sure you have:\n",
    "\n",
    "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
    "2. Basic familiarity with Python and Jupyter notebooks\n",
    "\n",
    "This notebook is designed to run in Google Colab, so no local Python installation is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPwW1Lw_7fFm"
   },
   "source": [
    "### 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F0TyWej7Z8L",
    "outputId": "9ecb6017-12b5-48c4-9f7d-cce35b7aa270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain_openai langchain-community faiss-cpu requests pypdf python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ev0mGaUbsCz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set the API key from Colab secrets\n",
    "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4r1JJZ1_U01"
   },
   "source": [
    "###Setup API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1II6I24s9JiU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set the API key from Colab secrets\n",
    "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glxZYmn67u0v"
   },
   "source": [
    "###  2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovSWsqyn7sB1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65EAkr_K7yvD"
   },
   "source": [
    "###  3. Function to Download PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9Kl7_wG74SD"
   },
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, save_path=\"downloaded.pdf\"):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Qdb89n75Lk"
   },
   "source": [
    "### 4. Function to Load Chat Model (Sutra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqD1w_mj78SB"
   },
   "outputs": [],
   "source": [
    "def get_chat_model():\n",
    "    return ChatOpenAI(\n",
    "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
    "        base_url=\"https://api.two.ai/v2\",\n",
    "        model=\"sutra-v2\",\n",
    "        temperature=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcwens7Z8wFG"
   },
   "source": [
    "###  5. Function to Process Documents into Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-TUETqi8zRw"
   },
   "outputs": [],
   "source": [
    "def process_documents(file_paths, chunk_size=1000, chunk_overlap=100):\n",
    "    documents = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Use OpenAI embeddings\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_chat_model(),\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaqCYiUx8279"
   },
   "source": [
    "### 6. Function to Query the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A6iBjOx87Lj"
   },
   "outputs": [],
   "source": [
    "def process_chat(conversation_chain, user_input, selected_language):\n",
    "    rag_response = conversation_chain.invoke(user_input)\n",
    "    context = rag_response[\"answer\"]\n",
    "\n",
    "    system_message = f\"\"\"\n",
    "    You are a helpful assistant that answers questions about documents.\n",
    "    Use the following context to answer the question.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    Please respond in {selected_language}.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=f\"{system_message}\\n\\nQuestion: {user_input}\")]\n",
    "    chat = get_chat_model()\n",
    "    response = chat.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYfBSko-891K"
   },
   "source": [
    "### 7. Example Usage Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iee7yfM19As5",
    "outputId": "8c205e8c-158e-4a70-be95-c80b14ae6d7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c8c1eabde0a9>:21: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "चिकन और गैलंगाल को नारियल के दूध में सूप (टॉम खा गाई) बनाने के लिए निम्नलिखित चरणों का पालन करें:\n",
      "\n",
      "### सामग्री (एक सर्विंग)\n",
      "- 150 ग्राम चिकन, छोटे टुकड़ों में कटा हुआ\n",
      "- 50 ग्राम युवा गैलंगाल, कटा हुआ\n",
      "- 100 ग्राम हल्का कुचला हुआ लेमनग्रास, जुलिएन में कटा हुआ\n",
      "- 100 ग्राम स्ट्रॉ मशरूम\n",
      "- 250 ग्राम नारियल का दूध\n",
      "- 100 ग्राम चिकन स्टॉक\n",
      "- 3 चम्मच नींबू का रस\n",
      "- 3 चम्मच मछली की सॉस\n",
      "- 2 पत्ते काफिर लाइम, कटी हुई\n",
      "- 1-2 बर्ड्स आई चिली, कुचली हुई\n",
      "- 3 पत्ते धनिया\n",
      "\n",
      "### तैयारी का समय: 10 मिनट\n",
      "### पकाने का समय: 20 मिनट\n",
      "### कुल समय: 30 मिनट\n",
      "\n",
      "### विधि:\n",
      "1. एक बर्तन में चिकन स्टॉक और नारियल का दूध धीमी आंच पर उबालें।\n",
      "2. बर्तन में कटे हुए गैलंगाल, लेमनग्रास, चिकन के टुकड़े और मशरूम डालें। जब सूप उबलने लगे, तो इसे मछली की सॉस के साथ सीज करें।\n",
      "3. जब चिकन पूरी तरह से पक जाए, तब इसमें कटी हुई काफिर लाइम की पत्तियाँ और कुचली हुई बर्ड्स आई चिली डालें।\n",
      "4. बर्तन को आंच से हटा दें और नींबू का रस मिलाएँ।\n",
      "5. परोसने से पहले सूप को धनिया की पत्तियों से सजाएँ।\n",
      "\n",
      "### टिप्स:\n",
      "- पकाने की प्रक्रिया के दौरान आंच को कम रखें ताकि नारियल के दूध का तेल अलग न हो।\n",
      "- यदि आप परिपक्व गैलंगाल का उपयोग कर रहे हैं, तो मात्रा कम करें।\n",
      "- बर्तन को आंच से हटाने के बाद नींबू का रस डालने से इसकी सुगंध बढ़ती है।\n",
      "- अगर आप हल्का स्वाद चाहते हैं तो मिर्च की संख्या कम कर सकते हैं।\n",
      "\n",
      "इस सूप का आनंद लें!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Download the PDF\n",
    "pdf_url = \"https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"\n",
    "downloaded_pdf_path = download_pdf(pdf_url)\n",
    "\n",
    "# STEP 2: Process documents\n",
    "conversation_chain = process_documents([downloaded_pdf_path])\n",
    "\n",
    "# STEP 3: Ask a question\n",
    "user_input = \"How do I make chicken and galangal in coconut milk soup\"\n",
    "selected_language = \"hindi\"\n",
    "\n",
    "# STEP 4: Get response\n",
    "answer = process_chat(conversation_chain, user_input, selected_language)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqciGnDF_u69"
   },
   "source": [
    "### Finally Integrated UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "de4fc6d4c6a14f1f8b7b95311bbeee67",
      "6d1db6f89a0143ba8382c141d821e79b",
      "33c52a49957f49da8f7cf55049aeeb06",
      "76895a0a37a145ec8085aee13740f1cb",
      "2d9c07487b5a4183aeb8f0c169752a84",
      "72bd48d4823b498dacf84361ff5d4af0",
      "dd9ee3d9790644efa8e6a79050a3b243",
      "8739a7126a2c489b9717190a6f907c18",
      "aa07f87c0e6641cd970295f286d85d22",
      "a389f305ea124d979d8c3687a2e69700",
      "57a84a6d4ec5423c98820cc6489ab900",
      "2810d2a31d674adea3cbcb9f798fcb07",
      "fe3c1f6d8ba4425fb2d9f45232dff12b",
      "6c5552e1a08a4fe4a395ad7b97fabc0e",
      "ec3931bd69284e66a9b81bf7ec4c6f43",
      "107271f5c17540158ef8dfb7219c7f27",
      "0ab451f76bed4028b0b05d5f8bb9a7a8",
      "d7e26e8934624de5b8bb862433a0368b",
      "54a3372eb555472fa6320be76f77dd23",
      "8f33bda9440b4bddaadd6798738bfb2a",
      "9ebcafe1979643e3bdb7fbbd2725d09a",
      "72e41f45d4c34716a4103f1d1e8ad6fc",
      "61d0b39816714be2a7b61dcf5cf8fec9",
      "687adfb489a2452e9ab4990090704ab7",
      "27a8ae78d6f742acbe696138a609693f",
      "b6798cbfd9234e60b4d08ee7a0f6f25f",
      "1926bf1d5c6d4b58a9fbf88a05e8445d",
      "0f96b9e245f84a14918773587d919a9e",
      "f0eb5b4961424e208daab9441d179577"
     ]
    },
    "id": "0z5kTlxM-Zww",
    "outputId": "a1898b78-7e6c-4210-91af-065668737870"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4fc6d4c6a14f1f8b7b95311bbeee67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h3 style='font-family:Arial;'>📚 Multilingual Chat with PDF (Powered by Sutra)</h3>…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "import requests\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import HumanMessage\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# 2. Setup LLM (Sutra)\n",
    "def get_sutra_model():\n",
    "    return ChatOpenAI(\n",
    "        api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
    "        base_url=\"https://api.two.ai/v2\",\n",
    "        model=\"sutra-v2\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# 3. Load and index PDF\n",
    "def load_and_index_pdf(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=get_sutra_model(),\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# 4. UI Components\n",
    "\n",
    "# PDF URL field\n",
    "pdf_url_input = widgets.Text(\n",
    "    placeholder='Enter PDF URL...',\n",
    "    description='📄 PDF URL:',\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "load_pdf_button = widgets.Button(\n",
    "    description=\"🔄 Load PDF\",\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "status_output = widgets.Output()\n",
    "\n",
    "# Language dropdown\n",
    "languages = [\n",
    "    \"English\", \"Hindi\", \"Gujarati\", \"Bengali\", \"Tamil\",\n",
    "    \"Telugu\", \"Kannada\", \"Malayalam\", \"Punjabi\", \"Marathi\",\n",
    "    \"Urdu\", \"Assamese\", \"Odia\", \"Sanskrit\", \"Korean\",\n",
    "    \"Japanese\", \"Arabic\", \"French\", \"German\", \"Spanish\",\n",
    "    \"Portuguese\", \"Russian\", \"Chinese\", \"Vietnamese\", \"Thai\",\n",
    "    \"Indonesian\", \"Turkish\", \"Polish\", \"Ukrainian\", \"Dutch\",\n",
    "    \"Italian\", \"Greek\", \"Hebrew\", \"Persian\", \"Swedish\",\n",
    "    \"Norwegian\", \"Danish\", \"Finnish\", \"Czech\", \"Hungarian\",\n",
    "    \"Romanian\", \"Bulgarian\", \"Croatian\", \"Serbian\", \"Slovak\",\n",
    "    \"Slovenian\", \"Estonian\", \"Latvian\", \"Lithuanian\", \"Malay\",\n",
    "    \"Tagalog\", \"Swahili\"\n",
    "]\n",
    "\n",
    "lang_dropdown = widgets.Dropdown(\n",
    "    options=languages,\n",
    "    value=\"English\",\n",
    "    description='🌐 Language:',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "chat_output = widgets.HTML(\n",
    "    value=\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>Chat history will appear here...</div>\"\n",
    ")\n",
    "\n",
    "user_input = widgets.Text(\n",
    "    placeholder='Type your message...',\n",
    "    layout=widgets.Layout(flex='4', width='auto')\n",
    ")\n",
    "\n",
    "send_button = widgets.Button(\n",
    "    description=\"📤 Send\",\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(flex='1', width='auto')\n",
    ")\n",
    "\n",
    "messages = []\n",
    "conversation_chain = None\n",
    "\n",
    "# 5. Load PDF Logic\n",
    "def download_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    tmp_file = NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
    "    with open(tmp_file.name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    return tmp_file.name\n",
    "\n",
    "def on_load_pdf(b):\n",
    "    global conversation_chain\n",
    "    pdf_url = pdf_url_input.value.strip()\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        if not pdf_url:\n",
    "            print(\"❌ Please enter a valid PDF URL.\")\n",
    "            return\n",
    "        print(\"⏳ Downloading and processing PDF...\")\n",
    "        try:\n",
    "            pdf_path = download_pdf(pdf_url)\n",
    "            conversation_chain = load_and_index_pdf(pdf_path)\n",
    "            print(\"✅ PDF loaded and indexed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error:\", e)\n",
    "\n",
    "load_pdf_button.on_click(on_load_pdf)\n",
    "\n",
    "# 6. Chat Interaction Logic\n",
    "def on_send_click(b):\n",
    "    global conversation_chain\n",
    "    if conversation_chain is None:\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            print(\"❌ Load a PDF first.\")\n",
    "        return\n",
    "\n",
    "    user_text = user_input.value.strip()\n",
    "    if not user_text:\n",
    "        return\n",
    "\n",
    "    lang = lang_dropdown.value\n",
    "    messages.append(f\"<b style='color:#13f22d;'>You:</b> {user_text}\")\n",
    "\n",
    "    context_response = conversation_chain.invoke(user_text)\n",
    "    rag_context = context_response['answer']\n",
    "\n",
    "    system_msg = f\"\"\"\n",
    "You are a helpful assistant answering based on a document.\n",
    "Use this context: {rag_context}\n",
    "Always reply in: {lang}\n",
    "Question: {user_text}\n",
    "\"\"\"\n",
    "\n",
    "    chat_model = get_sutra_model()\n",
    "    sutra_response = chat_model.invoke([HumanMessage(content=system_msg)])\n",
    "    assistant_reply = sutra_response.content.strip()\n",
    "\n",
    "    messages.append(f\"<b style='color:#007acc;'>Assistant ({lang}):</b> {assistant_reply}\")\n",
    "\n",
    "    chat_html = \"<br>\".join(messages)\n",
    "    chat_output.value = f\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>{chat_html}</div>\"\n",
    "\n",
    "    user_input.value = \"\"\n",
    "\n",
    "send_button.on_click(on_send_click)\n",
    "\n",
    "# 7. Final Layout\n",
    "input_row = widgets.HBox([user_input, send_button])\n",
    "pdf_row = widgets.HBox([pdf_url_input, load_pdf_button])\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3 style='font-family:Arial;'>📚 Multilingual Chat with PDF (Powered by Sutra)</h3>\"),\n",
    "    pdf_row,\n",
    "    lang_dropdown,\n",
    "    chat_output,\n",
    "    input_row,\n",
    "    status_output\n",
    "])\n",
    "\n",
    "# 8. Display the App\n",
    "display(ui)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
