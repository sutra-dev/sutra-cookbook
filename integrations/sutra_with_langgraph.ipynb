{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbo8swtGjquS"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://framerusercontent.com/images/9vH8BcjXKRcC5OrSfkohhSyDgX0.png\" width=\"130\">\n",
        "<img src=\"https://assets.streamlinehq.com/image/private/w_300,h_300,ar_1/f_auto/v1/icons/logos/langchain-ipuhh4qo1jz5ssl4x0g2a.png/langchain-dp1uxj2zn3752pntqnpfu2.png?_a=DATAdtAAZAA0\" width=\"140\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRAâ€™s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "  # LangGraph with SUTRA\n",
        "\n",
        "This notebook demonstrates how to use LangGraph with Sutra, a powerful language model from Two AI. We'll create a simple conversational agent with state management using LangGraph and Sutra's capabilities.\n",
        "\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1F4Lvv-iEEbR70s_RC3EBZWr9KA5UmmG7?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmc6rB30kBfC"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNrA-PrtGrfY"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install the required packages if they're not already installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJBjSrs7GrfZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-core langchain-community langgraph openai langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZqvPlwfGrfa"
      },
      "source": [
        "### Set Environment Variables (API Keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jid8XVm7kGxV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA5F3nUCGrfb"
      },
      "source": [
        "## Initialize Sutra Model\n",
        "\n",
        "We'll use the ChatOpenAI class to initialize the Sutra model with the appropriate base URL for Two AI's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5EkHbsJGrfb",
        "outputId": "9b9a2b75-28ae-420e-90d7-dda9d6ebd596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am an intelligent multilingual AI model designed to assist with a variety of inquiries and tasks. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
        "\n",
        "# Initialize the Sutra model\n",
        "chat = ChatOpenAI(\n",
        "    model=\"sutra-v2\",  # or \"sutra-r0\" for reasoning tasks\n",
        "    api_key=os.environ[\"SUTRA_API_KEY\"],\n",
        "    base_url=\"https://api.two.ai/v2\"\n",
        ")\n",
        "\n",
        "# Test the model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hello, who are you?\")\n",
        "]\n",
        "\n",
        "response = chat.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hspDTcnI49sx"
      },
      "source": [
        "###ðŸ§¬ Simple LangGraph Workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TfaOHahf6CY"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "\n",
        "# Define the state schema\n",
        "class GraphState(TypedDict):\n",
        "    messages: List[HumanMessage]\n",
        "\n",
        "# Define a node that processes the query with SUTRA-V2\n",
        "def chat_node(state: GraphState) -> GraphState:\n",
        "    messages = state[\"messages\"]\n",
        "    response = chat.invoke([\n",
        "        SystemMessage(content=\"You are a helpful AI that answers concisely in the requested language.\"),\n",
        "        *messages\n",
        "    ])\n",
        "    state[\"messages\"].append(response)\n",
        "    return state\n",
        "\n",
        "# Initialize the graph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add the chat node\n",
        "workflow.add_node(\"chat\", chat_node)\n",
        "\n",
        "# Set entry and exit points\n",
        "workflow.set_entry_point(\"chat\")\n",
        "workflow.add_edge(\"chat\", END)\n",
        "\n",
        "# Compile the graph\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Run the workflow\n",
        "def run_simple_workflow(query: str):\n",
        "    initial_state = {\"messages\": [HumanMessage(content=query)]}\n",
        "    result = graph.invoke(initial_state)\n",
        "    return result[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJiizVod5CXt"
      },
      "source": [
        "### Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhjsOovmgLD9"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "print(\"\\n=== Example 1: Simple Workflow ===\")\n",
        "response = run_simple_workflow(\"Explain the benefits of AI in education in Spanish.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUZujLc45K2s"
      },
      "source": [
        "###Advanced Reasoning with SUTRA-R0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I60O9dpygQDw"
      },
      "outputs": [],
      "source": [
        "# Initialize the Sutra model\n",
        "r0_chat = ChatOpenAI(\n",
        "    model=\"sutra-r0\",  # or \"sutra-r0\" for reasoning tasks\n",
        "    api_key=os.environ[\"SUTRA_API_KEY\"],\n",
        "    base_url=\"https://api.two.ai/v2\"\n",
        ")\n",
        "\n",
        "# Define the state schema for reasoning\n",
        "class ReasoningState(TypedDict):\n",
        "    messages: List[HumanMessage]\n",
        "    reasoning_steps: List[str]\n",
        "\n",
        "# Node for reasoning step\n",
        "def reasoning_node(state: ReasoningState) -> ReasoningState:\n",
        "    messages = state[\"messages\"]\n",
        "    reasoning_prompt = [\n",
        "        SystemMessage(content=\"You are a legal reasoning expert. Break down the query into logical steps before answering.\"),\n",
        "        *messages\n",
        "    ]\n",
        "    response = r0_chat.invoke(reasoning_prompt)\n",
        "    state[\"reasoning_steps\"].append(response.content)\n",
        "    state[\"messages\"].append(response)\n",
        "    return state\n",
        "\n",
        "# Initialize the graph\n",
        "reasoning_workflow = StateGraph(ReasoningState)\n",
        "\n",
        "# Add the reasoning node\n",
        "reasoning_workflow.add_node(\"reason\", reasoning_node)\n",
        "\n",
        "# Set entry and exit points\n",
        "reasoning_workflow.set_entry_point(\"reason\")\n",
        "reasoning_workflow.add_edge(\"reason\", END)\n",
        "\n",
        "# Compile the graph\n",
        "reasoning_graph = reasoning_workflow.compile()\n",
        "\n",
        "# Function to run reasoning workflow\n",
        "def run_reasoning(query: str):\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"reasoning_steps\": []\n",
        "    }\n",
        "    result = reasoning_graph.invoke(initial_state)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY_FPkeKqD4C"
      },
      "source": [
        "###Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGtl4ZDEp_Nk"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "print(\"\\n=== Example 3: Advanced Reasoning ===\")\n",
        "legal_query = \"If a contract states 'Party A is responsible unless Party B provides notice within 7 days,' who is liable if notice is given on day 8?\"\n",
        "result = run_reasoning(legal_query)\n",
        "print(\"Reasoning Steps:\", result[\"reasoning_steps\"])\n",
        "print(\"Final Answer:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpTGtmT5qziz"
      },
      "source": [
        "###Multilingual Capabilities with SUTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAN-JsOd4no0"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any, TypedDict\n",
        "import re\n",
        "\n",
        "# Define the state schema for multilingual processing\n",
        "class MultilingualState(TypedDict):\n",
        "    messages: List[Any]\n",
        "    detected_language: str\n",
        "    translations: Dict[str, str]\n",
        "\n",
        "# Function to detect language (simplified version)\n",
        "def detect_language(text: str) -> str:\n",
        "    # This is a simplified language detection\n",
        "    # In a real application, you would use a proper language detection library\n",
        "    patterns = {\n",
        "        \"es\": r'\\b(hola|como|estÃ¡|gracias|por favor)\\b',\n",
        "        \"fr\": r'\\b(bonjour|comment|merci|s\\'il vous plaÃ®t)\\b',\n",
        "        \"de\": r'\\b(hallo|wie|danke|bitte)\\b',\n",
        "        \"zh\": r'[\\u4e00-\\u9fff]',  # Chinese characters\n",
        "        \"hi\": r'[\\u0900-\\u097F]',  # Hindi characters\n",
        "        \"ja\": r'[\\u3040-\\u309F\\u30A0-\\u30FF]',  # Japanese characters\n",
        "    }\n",
        "\n",
        "    for lang, pattern in patterns.items():\n",
        "        if re.search(pattern, text.lower()):\n",
        "            return lang\n",
        "\n",
        "    return \"en\"  # Default to English\n",
        "\n",
        "# Node for language detection\n",
        "def language_detection_node(state: MultilingualState) -> MultilingualState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"detected_language\": state[\"detected_language\"],\n",
        "        \"translations\": state[\"translations\"].copy()\n",
        "    }\n",
        "\n",
        "    # Get the last user message\n",
        "    last_message = next((msg for msg in reversed(new_state[\"messages\"])\n",
        "                         if isinstance(msg, HumanMessage)), None)\n",
        "\n",
        "    if last_message:\n",
        "        # Detect the language\n",
        "        detected_lang = detect_language(last_message.content)\n",
        "        new_state[\"detected_language\"] = detected_lang\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Node for translation to English (if needed)\n",
        "def translation_node(state: MultilingualState) -> MultilingualState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"detected_language\": state[\"detected_language\"],\n",
        "        \"translations\": state[\"translations\"].copy()\n",
        "    }\n",
        "\n",
        "    # Get the last user message\n",
        "    last_message = next((msg for msg in reversed(new_state[\"messages\"])\n",
        "                         if isinstance(msg, HumanMessage)), None)\n",
        "\n",
        "    if last_message and state[\"detected_language\"] != \"en\":\n",
        "        # Translate to English using Sutra's multilingual capabilities\n",
        "        translation_prompt = [\n",
        "            SystemMessage(content=f\"Translate the following from {state['detected_language']} to English:\"),\n",
        "            HumanMessage(content=last_message.content)\n",
        "        ]\n",
        "\n",
        "        translation_response = chat.invoke(translation_prompt)\n",
        "\n",
        "        # Store the translation\n",
        "        new_state[\"translations\"][last_message.content] = translation_response.content\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Node for generating response in the original language\n",
        "def multilingual_response_node(state: MultilingualState) -> MultilingualState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"detected_language\": state[\"detected_language\"],\n",
        "        \"translations\": state[\"translations\"].copy()\n",
        "    }\n",
        "\n",
        "    # Get the last user message\n",
        "    last_message = next((msg for msg in reversed(new_state[\"messages\"])\n",
        "                         if isinstance(msg, HumanMessage)), None)\n",
        "\n",
        "    if last_message:\n",
        "        # Determine the content to respond to (original or translated)\n",
        "        query_content = last_message.content\n",
        "        if query_content in new_state[\"translations\"]:\n",
        "            # Use the English translation for processing\n",
        "            query_content = new_state[\"translations\"][query_content]\n",
        "\n",
        "        # Generate a response in the detected language\n",
        "        language_names = {\n",
        "            \"en\": \"English\",\n",
        "            \"es\": \"Spanish\",\n",
        "            \"fr\": \"French\",\n",
        "            \"de\": \"German\",\n",
        "            \"zh\": \"Chinese\",\n",
        "            \"hi\": \"Hindi\",\n",
        "            \"ja\": \"Japanese\"\n",
        "        }\n",
        "\n",
        "        lang_name = language_names.get(new_state[\"detected_language\"], \"the original language\")\n",
        "\n",
        "        response_prompt = [\n",
        "            SystemMessage(content=f\"You are a helpful assistant. Respond in {lang_name} to the following query:\"),\n",
        "            *new_state[\"messages\"]\n",
        "        ]\n",
        "\n",
        "        response = chat.invoke(response_prompt)\n",
        "        new_state[\"messages\"].append(response)\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Initialize the multilingual graph\n",
        "multilingual_builder = StateGraph(MultilingualState)\n",
        "\n",
        "# Add nodes\n",
        "multilingual_builder.add_node(\"detect_language\", language_detection_node)\n",
        "multilingual_builder.add_node(\"translate\", translation_node)\n",
        "multilingual_builder.add_node(\"generate_response\", multilingual_response_node)\n",
        "\n",
        "# Add edges\n",
        "multilingual_builder.add_edge(\"detect_language\", \"translate\")\n",
        "multilingual_builder.add_edge(\"translate\", \"generate_response\")\n",
        "multilingual_builder.add_edge(\"generate_response\", END)\n",
        "\n",
        "# Set entry point\n",
        "multilingual_builder.set_entry_point(\"detect_language\")\n",
        "\n",
        "# Compile the graph\n",
        "multilingual_graph = multilingual_builder.compile()\n",
        "\n",
        "# Function to interact with the multilingual system\n",
        "def multilingual_chat(query: str):\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"detected_language\": \"en\",  # Default\n",
        "        \"translations\": {}\n",
        "    }\n",
        "    result = multilingual_graph.invoke(initial_state)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8s2NY-_qcsi"
      },
      "source": [
        "###Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k5AlJhiqNd1"
      },
      "outputs": [],
      "source": [
        "# Spanish query\n",
        "spanish_query = \"Â¿Puedes explicarme cÃ³mo funciona la inteligencia artificial?\"\n",
        "spanish_result = multilingual_chat(spanish_query)\n",
        "print(f\"Query (Spanish): {spanish_query}\")\n",
        "print(f\"Detected Language: {spanish_result['detected_language']}\")\n",
        "print(f\"Response: {spanish_result['messages'][-1].content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjiyl7IsqT6D"
      },
      "outputs": [],
      "source": [
        "# Hindi query\n",
        "hindi_query = \"à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤®à¥à¤à¥‡ à¤¬à¤¤à¤¾à¤à¤‚\"\n",
        "hindi_result = multilingual_chat(hindi_query)\n",
        "print(f\"Query (Hindi): {hindi_query}\")\n",
        "print(f\"Detected Language: {hindi_result['detected_language']}\")\n",
        "print(f\"Response: {hindi_result['messages'][-1].content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDpLOG_sqYVH"
      },
      "outputs": [],
      "source": [
        "# English query\n",
        "english_query = \"Tell me about artificial intelligence\"\n",
        "english_result = multilingual_chat(english_query)\n",
        "print(f\"Query (English): {english_query}\")\n",
        "print(f\"Detected Language: {english_result['detected_language']}\")\n",
        "print(f\"Response: {english_result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD8S9jozt6Fg"
      },
      "source": [
        "###Interactive Agent with Feedback Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RT2W9n-q7_t"
      },
      "outputs": [],
      "source": [
        "# Define the state schema for feedback loop\n",
        "class FeedbackState(TypedDict):\n",
        "    messages: List[Any]\n",
        "    feedback: List[Dict[str, Any]]\n",
        "    quality_score: float\n",
        "\n",
        "# Node for generating initial response\n",
        "def generate_initial_response(state: FeedbackState) -> FeedbackState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"feedback\": state[\"feedback\"].copy(),\n",
        "        \"quality_score\": state[\"quality_score\"]\n",
        "    }\n",
        "\n",
        "    # Get the last user message\n",
        "    last_message = next((msg for msg in reversed(new_state[\"messages\"])\n",
        "                         if isinstance(msg, HumanMessage)), None)\n",
        "\n",
        "    if last_message:\n",
        "        system_prompt = \"You are a helpful assistant. Provide a concise and accurate response.\"\n",
        "        response = chat.invoke([SystemMessage(content=system_prompt)] + new_state[\"messages\"])\n",
        "        new_state[\"messages\"].append(response)\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Node for evaluating response quality\n",
        "def evaluate_response(state: FeedbackState) -> FeedbackState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"feedback\": state[\"feedback\"].copy(),\n",
        "        \"quality_score\": state[\"quality_score\"]\n",
        "    }\n",
        "\n",
        "    # Get the last AI message\n",
        "    last_ai_message = next((msg for msg in reversed(new_state[\"messages\"])\n",
        "                           if isinstance(msg, AIMessage)), None)\n",
        "\n",
        "    if last_ai_message:\n",
        "        # Use SUTRA-R0 to evaluate the response\n",
        "        evaluation_prompt = [\n",
        "            SystemMessage(content=\"\"\"You are a response evaluator. Rate the quality of the following response on a scale of 0.0 to 1.0.\n",
        "            Consider factors like accuracy, clarity, helpfulness, and conciseness.\n",
        "            Return ONLY a number between 0.0 and 1.0.\"\"\"),\n",
        "            HumanMessage(content=f\"Response to evaluate: {last_ai_message.content}\")\n",
        "        ]\n",
        "\n",
        "        evaluation = r0_chat.invoke(evaluation_prompt)\n",
        "\n",
        "        # Extract the score (assuming the model returns just a number)\n",
        "        try:\n",
        "            score_text = evaluation.content.strip()\n",
        "            score = float(score_text)\n",
        "            new_state[\"quality_score\"] = score\n",
        "        except ValueError:\n",
        "            # If the model didn't return just a number, try to extract it\n",
        "            import re\n",
        "            score_match = re.search(r'(\\d+\\.\\d+)', evaluation.content)\n",
        "            if score_match:\n",
        "                new_state[\"quality_score\"] = float(score_match.group(1))\n",
        "            else:\n",
        "                new_state[\"quality_score\"] = 0.5  # Default if parsing fails\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Node for improving response if needed\n",
        "def improve_response(state: FeedbackState) -> FeedbackState:\n",
        "    new_state = {\n",
        "        \"messages\": state[\"messages\"].copy(),\n",
        "        \"feedback\": state[\"feedback\"].copy(),\n",
        "        \"quality_score\": state[\"quality_score\"]\n",
        "    }\n",
        "\n",
        "    # If quality score is below threshold, improve the response\n",
        "    if new_state[\"quality_score\"] < 0.7:\n",
        "        # Get the last user and AI messages\n",
        "        last_user_message = next((msg for msg in reversed(new_state[\"messages\"][:-1])\n",
        "                                 if isinstance(msg, HumanMessage)), None)\n",
        "        last_ai_message = new_state[\"messages\"][-1]\n",
        "\n",
        "        if last_user_message and isinstance(last_ai_message, AIMessage):\n",
        "            improvement_prompt = [\n",
        "                SystemMessage(content=\"\"\"You are a response improver. The previous response was rated as needing improvement.\n",
        "                Please provide an improved version that is more accurate, clear, helpful, and concise.\"\"\"),\n",
        "                HumanMessage(content=f\"Original query: {last_user_message.content}\"),\n",
        "                HumanMessage(content=f\"Previous response: {last_ai_message.content}\"),\n",
        "                HumanMessage(content=f\"Quality score: {new_state['quality_score']}. Please improve this response.\")\n",
        "            ]\n",
        "\n",
        "            improved_response = chat.invoke(improvement_prompt)\n",
        "\n",
        "            # Record feedback\n",
        "            new_state[\"feedback\"].append({\n",
        "                \"original_response\": last_ai_message.content,\n",
        "                \"quality_score\": new_state[\"quality_score\"],\n",
        "                \"improved_response\": improved_response.content\n",
        "            })\n",
        "\n",
        "            # Replace the last message with the improved response\n",
        "            new_state[\"messages\"][-1] = AIMessage(content=improved_response.content)\n",
        "\n",
        "    return new_state\n",
        "\n",
        "# Decision node to determine if improvement is needed\n",
        "def needs_improvement(state: FeedbackState) -> str:\n",
        "    return \"improve_response\" if state[\"quality_score\"] < 0.7 else \"end\"\n",
        "\n",
        "# Initialize the feedback loop graph\n",
        "feedback_builder = StateGraph(FeedbackState)\n",
        "\n",
        "# Add nodes\n",
        "feedback_builder.add_node(\"generate_response\", generate_initial_response)\n",
        "feedback_builder.add_node(\"evaluate_response\", evaluate_response)\n",
        "feedback_builder.add_node(\"improve_response\", improve_response)\n",
        "\n",
        "# Add edges\n",
        "feedback_builder.add_edge(\"generate_response\", \"evaluate_response\")\n",
        "feedback_builder.add_conditional_edges(\n",
        "    \"evaluate_response\",\n",
        "    needs_improvement,\n",
        "    {\n",
        "        \"improve_response\": \"improve_response\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "feedback_builder.add_edge(\"improve_response\", END)\n",
        "\n",
        "# Set entry point\n",
        "feedback_builder.set_entry_point(\"generate_response\")\n",
        "\n",
        "# Compile the graph\n",
        "feedback_graph = feedback_builder.compile()\n",
        "\n",
        "# Function to get a response with quality feedback\n",
        "def get_quality_response(query: str):\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"feedback\": [],\n",
        "        \"quality_score\": 1.0  # Default score\n",
        "    }\n",
        "    result = feedback_graph.invoke(initial_state)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-GUDnf5rVoV"
      },
      "source": [
        "###Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxEQPySmrDJN"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "print(\"\\n=== Example 7: Interactive Agent with Feedback Loop ===\")\n",
        "\n",
        "# Simple query\n",
        "query = \"Explain quantum computing in simple terms\"\n",
        "result = get_quality_response(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Quality Score: {result['quality_score']}\")\n",
        "print(f\"Response: {result['messages'][-1].content}\")\n",
        "\n",
        "if result[\"feedback\"]:\n",
        "  print(\"\\nFeedback Applied:\")\n",
        "for fb in result[\"feedback\"]:\n",
        "  print(f\"Original Response: {fb['original_response']}\")\n",
        "  print(f\"Quality Score: {fb['quality_score']}\")\n",
        "  print(f\"Improved Response: {fb['improved_response']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
